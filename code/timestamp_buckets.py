#!/usr/bin/env python3
"""
æ—¶é—´æˆ³æ¡¶åŒ–åˆ†æè„šæœ¬
ä»åºåˆ—æ–‡ä»¶ä¸­æå–æ‰€æœ‰æ—¶é—´æˆ³ï¼ŒæŒ‰æ—¶é—´è·¨åº¦åˆ†æˆæŒ‡å®šæ•°é‡çš„æ¡¶
"""

import os
import pickle
import numpy as np
from pathlib import Path
from datetime import datetime
import argparse
import sys
import time
import ujson
from collections import Counter

def get_data_paths():
    """
    è·å–æ•°æ®è·¯å¾„é…ç½®
    """
    data_path = os.environ.get('USER_CACHE_PATH', './user_cache')
    
    return {
        'output_file': Path(data_path) / 'item_exposure' / 'timestamp_buckets.pkl',
        'item_count_file': Path(data_path) / 'item_exposure' / 'item_counts_per_bucket.pkl',
    }

def create_timestamp_buckets_by_time_span(seq_file_path, num_buckets=16384):
    """
    æŒ‰æ—¶é—´è·¨åº¦åˆ›å»ºæ¡¶ï¼ˆç­‰æ—¶é—´é—´éš”åˆ†æ¡¶ï¼Œå‘é‡åŒ–å®ç°ï¼‰

    - ä»…ç»Ÿè®¡ item è®°å½•ï¼ˆè·³è¿‡ user å ä½è¡Œï¼šitem_id ä¸º None æˆ– 0ï¼‰
    - ä½¿ç”¨ numpy ç›´æ–¹å›¾ä¸€æ¬¡æ€§ç»Ÿè®¡æ¯æ¡¶æ•°é‡ï¼Œé¿å… O(N*B)

    Args:
        seq_file_path (Path): åºåˆ—æ–‡ä»¶è·¯å¾„
        num_buckets (int): æ¡¶çš„æ•°é‡

    Returns:
        list: æ¡¶ä¿¡æ¯åˆ—è¡¨ï¼Œæ¯ä¸ªæ¡¶åŒ…å«åŒºé—´è¾¹ç•Œä¸è¯¥æ¡¶çš„æ—¶é—´æˆ³ä¸ªæ•°
    """
    print(f"ğŸš€ ç­‰æ—¶é—´é—´éš”åˆ†æ¡¶ï¼šè¯»å– {seq_file_path} å¹¶åˆ›å»º {num_buckets} ä¸ªæ¡¶...")
    start_time = time.time()

    timestamps = []
    line_count = 0

    try:
        with open(seq_file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    user_sequence = ujson.loads(line)
                    for record in user_sequence:
                        if len(record) < 6:
                            continue
                        item_id = record[1]
                        ts = record[5]
                        # ä»…ä¿ç•™ item è¡Œ
                        if (item_id is None) or (item_id == 0):
                            continue
                        if ts is None or ts <= 0:
                            continue
                        timestamps.append(ts)
                    line_count += 1
                    if line_count % 1000000 == 0:
                        elapsed = max(1e-9, time.time() - start_time)
                        print(f"  å·²å¤„ç† {line_count} è¡Œï¼Œé€Ÿåº¦ {line_count/elapsed:.1f} è¡Œ/ç§’ï¼Œç´¯è®¡æ—¶é—´æˆ³ {len(timestamps)}")
                except Exception as e:
                    # è·³è¿‡å¼‚å¸¸è¡Œ
                    continue
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: åºåˆ—æ–‡ä»¶ {seq_file_path} æœªæ‰¾åˆ°")
        return []
    except Exception as e:
        print(f"âŒ å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return []

    if not timestamps:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ—¶é—´æˆ³")
        return []

    ts_np = np.asarray(timestamps, dtype=np.float64)
    min_ts = float(ts_np.min())
    max_ts = float(ts_np.max())

    print(f"ğŸ“… æ—¶é—´æˆ³èŒƒå›´: {datetime.fromtimestamp(min_ts)} åˆ° {datetime.fromtimestamp(max_ts)}")
    print(f"â±ï¸  æ€»æ—¶é—´è·¨åº¦: {(max_ts - min_ts) / 86400:.2f} å¤©")

    if num_buckets <= 0:
        num_buckets = 1
    # ç”Ÿæˆç­‰è·è¾¹ç•Œï¼ˆåŒ…å«å³ç«¯ç‚¹ï¼‰ï¼Œé•¿åº¦ num_buckets+1
    edges = np.linspace(min_ts, max_ts, num_buckets + 1, dtype=np.float64)
    # ä½¿ç”¨ç›´æ–¹å›¾ç»Ÿè®¡æ¯ä¸ªåŠå¼€åŒºé—´ [edges[i], edges[i+1]) çš„æ•°é‡ï¼ˆæœ€åä¸€æ¡¶åŒ…å«å³ç«¯ç‚¹ï¼‰
    counts, _ = np.histogram(ts_np, bins=edges)
    
    buckets = []
    for i in range(num_buckets):
        start_ts = float(edges[i])
        end_ts = float(edges[i + 1])
        # æ—¶é—´è·¨åº¦ï¼ˆå°æ—¶ï¼‰
        span_hours = max(0.0, (end_ts - start_ts) / 3600.0)
        buckets.append({
            'bucket_id': i,
            'start_timestamp': start_ts,
            'end_timestamp': end_ts,
            'start_datetime': datetime.fromtimestamp(start_ts).isoformat(),
            'end_datetime': datetime.fromtimestamp(end_ts).isoformat(),
            'timestamp_count': int(counts[i]),
            'time_span_hours': span_hours,
        })

    print(f"âœ… åˆ†æ¡¶å®Œæˆï¼Œå…± {num_buckets} ä¸ªæ¡¶ï¼Œéç©ºæ¡¶ {int((counts>0).sum())} ä¸ª")
    print(f"  æ¯æ¡¶å¹³å‡ {counts.mean():.2f}ï¼Œæœ€å¤§ {counts.max()}ï¼Œæœ€å° {counts.min()}")
    print(f"â±ï¸  æ€»ç”¨æ—¶: {time.time() - start_time:.1f}ç§’")

    return buckets

def save_buckets(buckets, output_file):
    """
    ä¿å­˜æ¡¶ä¿¡æ¯åˆ°æ–‡ä»¶
    
    Args:
        buckets (list): æ¡¶ä¿¡æ¯åˆ—è¡¨
        output_file (Path): è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    print("ğŸ’¾ æ­£åœ¨ä¿å­˜æ¡¶ä¿¡æ¯...")
    save_start = time.time()
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    try:
        with open(output_file, 'wb') as f:
            pickle.dump(buckets, f, protocol=pickle.HIGHEST_PROTOCOL)
        
        save_time = time.time() - save_start
        print(f"âœ… æ¡¶ä¿¡æ¯å·²ä¿å­˜åˆ°: {output_file}ï¼Œä¿å­˜ç”¨æ—¶: {save_time:.1f}ç§’")
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        non_empty_buckets = [b for b in buckets if b['timestamp_count'] > 0]
        print(f"\nğŸ“Š æ¡¶ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  æ€»æ¡¶æ•°: {len(buckets)}")
        print(f"  éç©ºæ¡¶æ•°: {len(non_empty_buckets)}")
        print(f"  æ€»æ—¶é—´æˆ³æ•°: {sum(b['timestamp_count'] for b in buckets)}")
        
        if non_empty_buckets:
            total_days = (buckets[-1]['end_timestamp'] - buckets[0]['start_timestamp']) / 86400
            print(f"  æ—¶é—´èŒƒå›´: {buckets[0]['start_datetime']} åˆ° {buckets[-1]['end_datetime']}")
            print(f"  æ€»å¤©æ•°: {total_days:.2f} å¤©")
            avg_time_span = np.mean([b['time_span_hours'] for b in buckets if b['time_span_hours'] > 0])
            print(f"  å¹³å‡æ¯æ¡¶æ—¶é—´è·¨åº¦: {avg_time_span:.2f} å°æ—¶")
            counts = [b['timestamp_count'] for b in non_empty_buckets]
            print(f"  æ—¶é—´æˆ³åˆ†å¸ƒ: æœ€å° {min(counts)}, æœ€å¤§ {max(counts)}, "
                  f"å¹³å‡ {np.mean(counts):.1f}")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜æ¡¶ä¿¡æ¯å¤±è´¥: {e}")

def create_timestamp_buckets_by_frequency(seq_file_path, num_buckets=16384):
    """
    ç­‰é¢‘åˆ†æ¡¶ï¼ˆåŸç‰ˆï¼Œä¸¤æ¬¡æ–‡ä»¶è¯»å–ï¼‰

    ä¸¤é˜¶æ®µï¼š
    1) ç¬¬ä¸€æ¬¡è¯»å–ï¼Œæå–å…¨éƒ¨æ—¶é—´æˆ³å¹¶æ’åºï¼Œæ„å»ºåˆ†ä½è¾¹ç•Œ(boundaries)
    2) ç¬¬äºŒæ¬¡è¯»å–ï¼Œä¾æ®è¾¹ç•Œå°†è®°å½•æ˜ å°„åˆ°æ¡¶ï¼Œç»Ÿè®¡æ¯ä¸ªæ¡¶å†…çš„ item å‡ºç°æ¬¡æ•°

    Args:
        seq_file_path (Path): åºåˆ—æ–‡ä»¶è·¯å¾„
        num_buckets (int): æ¡¶çš„æ•°é‡

    Returns:
        tuple[list, list[dict]]: (æ¡¶å…ƒæ•°æ®åˆ—è¡¨, æ¯æ¡¶ item->count æ˜ å°„åˆ—è¡¨)
    """
    print(f"ğŸš€ [åŸç‰ˆæ–¹æ³•] å¼€å§‹ç­‰é¢‘åˆ†æ¡¶å¹¶ç»Ÿè®¡æ¯æ¡¶ item æ¬¡æ•°ï¼Œç›®æ ‡æ¡¶æ•°: {num_buckets} ...")
    global_start = time.time()

    # ç¬¬ä¸€é˜¶æ®µï¼šæ”¶é›†å¹¶æ’åºæ‰€æœ‰æ—¶é—´æˆ³
    timestamps = []
    line_count = 0
    try:
        with open(seq_file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    user_sequence = ujson.loads(line)
                    for record in user_sequence:
                        if len(record) >= 6:
                            item_id = record[1]
                            if (item_id is None) or (item_id == 0):
                                continue
                            timestamp = record[5]
                            if timestamp is not None and timestamp > 0:
                                timestamps.append(timestamp)
                    line_count += 1
                    if line_count % 100000 == 0:
                        elapsed = time.time() - global_start
                        speed = line_count / max(elapsed, 1e-9)
                        print(f"  [é˜¶æ®µ1] å·²å¤„ç† {line_count} è¡Œï¼Œé€Ÿåº¦: {speed:.1f} è¡Œ/ç§’ï¼Œæ—¶é—´æˆ³æ•°é‡: {len(timestamps)}")
                except Exception as e:
                    print(f"å¤„ç†è¡Œæ—¶å‡ºé”™: {line[:100]}..., é”™è¯¯: {e}")
                    continue
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: åºåˆ—æ–‡ä»¶ {seq_file_path} æœªæ‰¾åˆ°")
        return [], []
    
    if not timestamps:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ—¶é—´æˆ³")
        return [], []

    print(f"ğŸ“Š æ€»å…±æ”¶é›†åˆ° {len(timestamps)} ä¸ªæ—¶é—´æˆ³")
    print("ğŸ”„ æ­£åœ¨æ’åºæ—¶é—´æˆ³ç”¨äºç­‰é¢‘åˆ†æ¡¶...")
    sort_start = time.time()
    timestamps.sort()
    print(f"âœ… æ’åºå®Œæˆï¼Œç”¨æ—¶: {time.time() - sort_start:.1f}ç§’")

    total = len(timestamps)
    num_buckets = max(1, min(num_buckets, total))

    # æ„å»ºè¾¹ç•Œ
    boundaries = [timestamps[int(i * total / num_buckets)] for i in range(num_buckets)]
    boundaries.append(timestamps[-1] + 1)

    buckets = []
    for i in range(num_buckets):
        start_ts = boundaries[i]
        end_ts_inclusive = boundaries[i + 1] - 1
        buckets.append({
            'bucket_id': i, 'start_timestamp': start_ts, 'end_timestamp': end_ts_inclusive,
            'start_datetime': datetime.fromtimestamp(start_ts).isoformat(),
            'end_datetime': datetime.fromtimestamp(end_ts_inclusive).isoformat(),
            'timestamp_count': 0, 'time_span_hours': max(0.0, (end_ts_inclusive - start_ts) / 3600)
        })

    # ç¬¬äºŒé˜¶æ®µï¼šç»Ÿè®¡
    print("ğŸ“¦ æ­£åœ¨æŒ‰è¾¹ç•Œç»Ÿè®¡æ¯æ¡¶ item æ¬¡æ•°...")
    import bisect
    counts_per_bucket = [Counter() for _ in range(num_buckets)]
    
    line_count = 0
    start_phase2 = time.time()
    with open(seq_file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip();
            if not line: continue
            try:
                user_sequence = ujson.loads(line)
                for record in user_sequence:
                    if len(record) >= 6:
                        item_id = record[1]
                        if (item_id is None) or (item_id == 0): continue
                        ts = record[5]
                        if ts is None or ts <= 0: continue
                        b_idx = bisect.bisect_right(boundaries, ts) - 1
                        b_idx = max(0, min(b_idx, num_buckets - 1))
                        counts_per_bucket[b_idx][item_id] += 1
                        buckets[b_idx]['timestamp_count'] += 1
                line_count += 1
                if line_count % 100000 == 0:
                    elapsed = time.time() - start_phase2
                    speed = line_count / max(elapsed, 1e-9)
                    print(f"  [é˜¶æ®µ2] å·²å¤„ç† {line_count} è¡Œï¼Œé€Ÿåº¦: {speed:.1f} è¡Œ/ç§’")
            except Exception: continue

    total_time = time.time() - global_start
    print(f"âœ… ç­‰é¢‘åˆ†æ¡¶ä¸ç»Ÿè®¡å®Œæˆï¼Œæ€»ç”¨æ—¶: {total_time:.1f}ç§’")
    return buckets, [dict(c) for c in counts_per_bucket]

def create_timestamp_buckets_by_frequency_accelerated(seq_file_path, num_buckets=32768):
    """
    ç­‰é¢‘åˆ†æ¡¶åŠ é€Ÿç‰ˆ (å•æ¬¡è¯»å–)

    é€šè¿‡ä¸€æ¬¡æ€§å°† (timestamp, item_id) è¯»å…¥å†…å­˜å¹¶æ’åºï¼Œé¿å…å¯¹å¤§æ–‡ä»¶è¿›è¡Œç¬¬äºŒæ¬¡IOæ‰«æï¼Œ
    ä»è€Œå¤§å¹…æå‡å¤„ç†é€Ÿåº¦ã€‚

    æ³¨æ„ï¼šæ­¤æ–¹æ³•ä¼šæ¶ˆè€—æ›´å¤šå†…å­˜ï¼Œå› ä¸ºå®ƒéœ€è¦å­˜å‚¨æ‰€æœ‰çš„ (timestamp, item_id) å¯¹ã€‚
    å¦‚æœå†…å­˜ä¸è¶³ï¼ŒåŸæœ‰çš„åŒæ¬¡æ‰«ææ–¹æ³•å¯èƒ½æ›´é€‚ç”¨ã€‚

    æµç¨‹:
    1) è¯»å–æ‰€æœ‰ item è®°å½•çš„ (timestamp, item_id) åˆ°å†…å­˜ã€‚
    2) åŸºäº timestamp å¯¹è®°å½•è¿›è¡Œæ’åºã€‚
    3) å°†æ’åºåçš„è®°å½•åˆ—è¡¨æŒ‰æ•°é‡å¹³å‡åˆ‡åˆ†æˆ num_buckets ä»½ã€‚
    4) ä¸ºæ¯ä¸ªåˆ‡ç‰‡ç”Ÿæˆæ¡¶ä¿¡æ¯å¹¶ç»Ÿè®¡ item å‡ºç°æ¬¡æ•°ã€‚

    Args:
        seq_file_path (Path): åºåˆ—æ–‡ä»¶è·¯å¾„
        num_buckets (int): æ¡¶çš„æ•°é‡

    Returns:
        tuple[list, list[dict]]: (æ¡¶å…ƒæ•°æ®åˆ—è¡¨, æ¯æ¡¶ item->count æ˜ å°„åˆ—è¡¨)
    """
    print(f"ğŸš€ [åŠ é€Ÿç‰ˆ] å¼€å§‹ç­‰é¢‘åˆ†æ¡¶ï¼Œç›®æ ‡æ¡¶æ•°: {num_buckets} ...")
    global_start = time.time()

    # 1. ä¸€æ¬¡æ€§è¯»å–æ‰€æœ‰ (timestamp, item_id) å¯¹
    records = []
    line_count = 0
    print("  [é˜¶æ®µ1/3] æ­£åœ¨è¯»å–æ‰€æœ‰è®°å½•åˆ°å†…å­˜...")
    try:
        with open(seq_file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    user_sequence = ujson.loads(line)
                    for record in user_sequence:
                        if len(record) >= 6:
                            item_id = record[1]
                            if (item_id is None) or (item_id == 0):
                                continue
                            timestamp = record[5]
                            if timestamp is not None and timestamp > 0:
                                records.append((timestamp, item_id))
                    line_count += 1
                    if line_count % 100000 == 0:
                        elapsed = time.time() - global_start
                        speed = line_count / max(elapsed, 1e-9)
                        print(f"    å·²å¤„ç† {line_count} è¡Œï¼Œé€Ÿåº¦: {speed:.1f} è¡Œ/ç§’ï¼Œè®°å½•æ•°: {len(records)}")
                except Exception:
                    # è·³è¿‡æ ¼å¼é”™è¯¯çš„è¡Œ
                    continue
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: åºåˆ—æ–‡ä»¶ {seq_file_path} æœªæ‰¾åˆ°")
        return [], []
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return [], []

    if not records:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆè®°å½•")
        return [], []

    print(f"  âœ… è¯»å–å®Œæˆï¼Œæ€»å…±æ”¶é›†åˆ° {len(records)} æ¡è®°å½•ã€‚")

    # 2. æ’åº
    print("  [é˜¶æ®µ2/3] æ­£åœ¨åŸºäºæ—¶é—´æˆ³æ’åºè®°å½•...")
    sort_start = time.time()
    records.sort(key=lambda x: x[0]) # æŒ‰æ—¶é—´æˆ³æ’åº
    print(f"  âœ… æ’åºå®Œæˆï¼Œç”¨æ—¶: {time.time() - sort_start:.1f}ç§’")

    # 3. åˆ†æ¡¶ä¸ç»Ÿè®¡
    print(f"  [é˜¶æ®µ3/3] æ­£åœ¨åˆ›å»º {num_buckets} ä¸ªæ¡¶å¹¶ç»Ÿè®¡ item æ¬¡æ•°...")
    bucketing_start = time.time()
    
    buckets = []
    item_counts_per_bucket = []
    
    total_records = len(records)
    num_buckets = max(1, min(num_buckets, total_records))

    for i in range(num_buckets):
        # è®¡ç®—å½“å‰æ¡¶åœ¨ records åˆ—è¡¨ä¸­çš„èµ·æ­¢ç´¢å¼•
        start_index = int(i * total_records / num_buckets)
        end_index = int((i + 1) * total_records / num_buckets)
        
        # è·å–å½“å‰æ¡¶çš„è®°å½•åˆ‡ç‰‡
        bucket_slice = records[start_index:end_index]
        
        if not bucket_slice:
            start_ts = buckets[-1]['end_timestamp'] if buckets else records[0][0]
            item_counts = {}
            count = 0
            end_ts = start_ts
        else:
            # ä½¿ç”¨ç”Ÿæˆå™¨è¡¨è¾¾å¼å’Œ Counter é«˜æ•ˆç»Ÿè®¡
            item_counts = Counter(rec[1] for rec in bucket_slice)
            start_ts = bucket_slice[0][0]
            end_ts = bucket_slice[-1][0]
            count = len(bucket_slice)

        buckets.append({
            'bucket_id': i,
            'start_timestamp': start_ts,
            'end_timestamp': end_ts,
            'start_datetime': datetime.fromtimestamp(start_ts).isoformat(),
            'end_datetime': datetime.fromtimestamp(end_ts).isoformat(),
            'timestamp_count': count,
            'time_span_hours': max(0.0, (end_ts - start_ts) / 3600)
        })
        item_counts_per_bucket.append(dict(item_counts))

    print(f"  âœ… åˆ†æ¡¶ä¸ç»Ÿè®¡å®Œæˆï¼Œç”¨æ—¶: {time.time() - bucketing_start:.1f}ç§’")
    total_time = time.time() - global_start
    print(f"âœ… [åŠ é€Ÿç‰ˆ] ç­‰é¢‘åˆ†æ¡¶ä¸ç»Ÿè®¡å®Œæˆï¼Œæ€»ç”¨æ—¶: {total_time:.1f}ç§’")
    
    return buckets, item_counts_per_bucket

def save_item_counts(item_counts_per_bucket, output_file):
    """
    ä¿å­˜æ¯ä¸ªæ¡¶å†…çš„ item è®¡æ•°å­—å…¸åˆ—è¡¨åˆ°æ–‡ä»¶

    Args:
        item_counts_per_bucket (list[dict]): æ¯ä¸ªæ¡¶ä¸€ä¸ªå­—å…¸ï¼Œé”®ä¸º item_idï¼Œå€¼ä¸ºæ¬¡æ•°
        output_file (Path): è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    print("ğŸ’¾ æ­£åœ¨ä¿å­˜æ¯æ¡¶ item è®¡æ•°...")
    save_start = time.time()
    output_file.parent.mkdir(parents=True, exist_ok=True)
    try:
        with open(output_file, 'wb') as f:
            pickle.dump(item_counts_per_bucket, f, protocol=pickle.HIGHEST_PROTOCOL)
        print(f"âœ… item è®¡æ•°å·²ä¿å­˜åˆ°: {output_file}ï¼Œä¿å­˜ç”¨æ—¶: {time.time() - save_start:.1f}ç§’")
    except Exception as e:
        print(f"âŒ ä¿å­˜ item è®¡æ•°å¤±è´¥: {e}")

def print_bucket_preview(buckets, preview_count=10):
    """
    æ‰“å°æ¡¶ä¿¡æ¯é¢„è§ˆ
    
    Args:
        buckets (list): æ¡¶ä¿¡æ¯åˆ—è¡¨
        preview_count (int): é¢„è§ˆçš„æ¡¶æ•°é‡
    """
    if not buckets:
        return
    print(f"\n--- æ¡¶ä¿¡æ¯é¢„è§ˆ (å‰{preview_count}ä¸ªæ¡¶) ---")
    
    for i, bucket in enumerate(buckets[:preview_count]):
        print(f"æ¡¶ {bucket['bucket_id']}: {bucket['start_datetime']} - {bucket['end_datetime']} "
              f"({bucket['timestamp_count']} ä¸ªæ—¶é—´æˆ³, {bucket['time_span_hours']:.2f} å°æ—¶)")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='æ—¶é—´æˆ³æ¡¶åŒ–åˆ†æè„šæœ¬ï¼Œé€šè¿‡å•æ¬¡è¯»å–å†…å­˜æ’åºåŠ é€Ÿç­‰é¢‘åˆ†æ¡¶ã€‚'
    )
    parser.add_argument(
        '--seq_file',
        type=str,
        help='åºåˆ—æ–‡ä»¶è·¯å¾„'
    )
    parser.add_argument(
        '--output_file',
        type=str,
        help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæœªæŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„'
    )
    parser.add_argument(
        '--buckets',
        type=int,
        default=8192,  # ä¿®æ”¹ï¼šé»˜è®¤æ¡¶æ•°è®¾ç½®ä¸º 32k
        help='æ¡¶çš„æ•°é‡ (é»˜è®¤: 32768)'
    )
    parser.add_argument(
        '--method',
        choices=['accelerated_frequency', 'frequency', 'timespan'], # ä¿®æ”¹ï¼šæ·»åŠ æ–°æ–¹æ³•å¹¶è®¾ä¸ºé»˜è®¤
        default='frequency',
        help='åˆ†æ¡¶æ–¹æ³•: accelerated_frequency(å•æ¬¡è¯»å–åŠ é€Ÿç­‰é¢‘ï¼Œæ¨è) / frequency(åŸç‰ˆç­‰é¢‘) / timespan(ç­‰æ—¶é—´è·¨åº¦)'
    )
    parser.add_argument(
        '--item_count_file',
        type=str,
        help='æ¯æ¡¶ item è®¡æ•°è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆä¸æŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„ï¼‰'
    )
    
    args = parser.parse_args()
    
    # è·å–è·¯å¾„
    if args.seq_file:
        seq_file_path = Path(args.seq_file)
    else:
        data_path = os.environ.get('TRAIN_DATA_PATH', './data')
        seq_file_path = Path(data_path) / 'seq.jsonl'
    
    if args.output_file:
        output_file = Path(args.output_file)
    else:
        paths = get_data_paths()
        output_file = paths['output_file']

    if args.item_count_file:
        item_count_file = Path(args.item_count_file)
    else:
        paths = get_data_paths()
        item_count_file = paths['item_count_file']
    
    print("="*60)
    print("=== æ—¶é—´æˆ³æ¡¶åŒ–åˆ†æè„šæœ¬ ===")
    print(f"åºåˆ—æ–‡ä»¶: {seq_file_path}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
    print(f"æ¡¶æ•°é‡: {args.buckets}")
    print(f"åˆ†æ¡¶æ–¹æ³•: {args.method}")
    if 'frequency' in args.method:
        print(f"itemè®¡æ•°è¾“å‡º: {item_count_file}")
    print("="*60)
    
    if not seq_file_path.exists():
        print(f"âŒ é”™è¯¯: åºåˆ—æ–‡ä»¶ä¸å­˜åœ¨ {seq_file_path}")
        sys.exit(1)
    
    # æ ¹æ®é€‰æ‹©çš„æ–¹æ³•åˆ›å»ºæ¡¶
    item_counts = None
    if args.method == 'accelerated_frequency':
        buckets, item_counts = create_timestamp_buckets_by_frequency_accelerated(seq_file_path, args.buckets)
    elif args.method == 'frequency':
        buckets, item_counts = create_timestamp_buckets_by_frequency(seq_file_path, args.buckets)
    elif args.method == 'timespan':
        buckets = create_timestamp_buckets_by_time_span(seq_file_path, args.buckets)
    else:
        print(f"âŒ é”™è¯¯ï¼šæœªçŸ¥çš„åˆ†æ¡¶æ–¹æ³• '{args.method}'")
        sys.exit(1)

    if not buckets:
        print("âŒ æœªèƒ½åˆ›å»ºä»»ä½•æ¡¶ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
        sys.exit(1)

    # ä¿å­˜ç»“æœ
    save_buckets(buckets, output_file)
    if item_counts is not None:
        save_item_counts(item_counts, item_count_file)

    print_bucket_preview(buckets)
    
    print("\nğŸ¯ æ—¶é—´æˆ³æ¡¶åŒ–åˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main()