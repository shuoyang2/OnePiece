#!/usr/bin/env python3
"""
æœ¬è„šæœ¬ç”¨äºåˆ†æç”¨æˆ·è¡Œä¸ºåºåˆ—æ•°æ®ï¼Œè®¡ç®—æ¯ä¸ªç‰©å“çš„å…³é”®æŒ‡æ ‡ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
æ€§èƒ½ä¼˜åŒ–ï¼š
1. ä½¿ç”¨ujsonæ›¿ä»£jsonåŠ é€Ÿè§£æ
2. æ‰¹é‡å¤„ç†å‡å°‘å¾ªç¯å¼€é”€
3. ä¼˜åŒ–æ—¥æœŸè½¬æ¢ç¼“å­˜
4. ä½¿ç”¨numpyå‘é‡åŒ–æ“ä½œ
5. å‡å°‘å†…å­˜ä½¿ç”¨
6. æ·»åŠ è¿›åº¦ç›‘æ§
"""

import os
import numpy as np
import pickle
from datetime import datetime, date
from pathlib import Path
from collections import defaultdict, Counter
import argparse
import sys
import time
import ujson  # æ›´å¿«çš„JSONè§£æåº“
import mmap  # å†…å­˜æ˜ å°„æ–‡ä»¶åŠ é€Ÿè¯»å–
import platform  # æ£€æµ‹æ“ä½œç³»ç»Ÿ


# =============================================================================
# æ ¸å¿ƒåŠŸèƒ½æ¨¡å—
# =============================================================================

def get_data_paths():
    """
    è·å–å¹¶é…ç½®æ•°æ®è¾“å…¥å’Œè¾“å‡ºè·¯å¾„ã€‚
    é€šè¿‡ç¯å¢ƒå˜é‡ TRAIN_DATA_PATH å’Œ USER_CACHE_PATH è¿›è¡Œé…ç½®ï¼Œå¦‚æœæœªè®¾ç½®åˆ™ä½¿ç”¨é»˜è®¤å€¼ã€‚
    """
    data_path = os.environ.get('TRAIN_DATA_PATH', './data')
    output_path = os.environ.get('USER_CACHE_PATH', './user_cache')

    return {
        'seq_file': Path(data_path) / 'seq.jsonl',
        'output_dir': Path(output_path) / 'item_exposure',
    }


def analyze_item_actions(seq_file_path, output_dir):
    """
    åˆ†ææ¯ä¸ªitemçš„æ›å…‰ã€ç‚¹å‡»ã€è½¬åŒ–è¡Œä¸ºï¼Œå¹¶è®¡ç®—åœ¨å¹³å‡æ›å…‰æ—¥çš„ç›¸å…³æŒ‡æ ‡ï¼ˆä¼˜åŒ–ç‰ˆï¼‰

    Args:
        seq_file_path (Path): è¡Œä¸ºåºåˆ—æ•°æ®æ–‡ä»¶ (seq.jsonl) çš„è·¯å¾„ã€‚
        output_dir (Path): è¾“å‡ºç»“æœæ–‡ä»¶çš„å­˜æ”¾ç›®å½•ã€‚
    """
    print("ğŸš€ å¼€å§‹åˆ†æç‰©å“çš„æ›å…‰ã€ç‚¹å‡»å’Œè½¬åŒ–è¡Œä¸º (ä¼˜åŒ–ç‰ˆ)...")
    start_time = time.time()

    # ä½¿ç”¨æ›´é«˜æ•ˆçš„æ•°æ®ç»“æ„
    item_stats = defaultdict(lambda: {
        'all_timestamps': [],
        'exposures': [],
        'clicks': [],
        'conversions': []
    })

    # ä½¿ç”¨Counterè¿›è¡Œå¿«é€Ÿè®¡æ•°
    item_daily_counts = defaultdict(lambda: defaultdict(Counter))
    global_daily_counts = defaultdict(Counter)

    # action_type åˆ°è¡Œä¸ºåç§°çš„æ˜ å°„
    action_map = {0: 'exposures', 1: 'clicks', 2: 'conversions'}

    # æ—¥æœŸè½¬æ¢ç¼“å­˜
    date_cache = {}

    def get_date_from_timestamp(ts):
        """ç¼“å­˜æ—¥æœŸè½¬æ¢ç»“æœ"""
        if ts not in date_cache:
            # ç®€åŒ–æ—¥æœŸè½¬æ¢ï¼Œåªä¿ç•™æ—¥æœŸéƒ¨åˆ†
            date_cache[ts] = date.fromtimestamp(ts)
        return date_cache[ts]

    # æ‰¹é‡å¤„ç†å¤§å°
    BATCH_SIZE = 1000000
    processed_lines = 0
    processed_records = 0
    line_count = 0

    try:
        with open(seq_file_path, 'r', encoding='utf-8') as f:
            # ä½¿ç”¨å†…å­˜æ˜ å°„åŠ é€Ÿè¯»å–
            if platform.system() == 'Windows':
                # Windowséœ€è¦ç‰¹æ®Šå¤„ç†
                mmapped_file = mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)
            else:
                # Linux/macOSå¤„ç†
                mmapped_file = mmap.mmap(f.fileno(), 0, prot=mmap.PROT_READ)

            batch_lines = []

            # ä½¿ç”¨è¿­ä»£å™¨è¯»å–è¡Œ
            pos = 0
            mmapped_file.seek(0)
            while True:
                line = mmapped_file.readline()
                if not line:
                    break

                try:
                    # è§£ç å­—èŠ‚ä¸ºå­—ç¬¦ä¸²
                    decoded_line = line.decode('utf-8').strip()
                    if not decoded_line:
                        continue

                    batch_lines.append(decoded_line)
                    line_count += 1

                    # æ¯50ä¸‡è¡Œæ‰“å°ä¸€æ¬¡è¿›åº¦
                    if line_count % 500000 == 0:
                        elapsed = time.time() - start_time
                        speed = line_count / elapsed
                        print(f"  å·²å¤„ç† {line_count} è¡Œï¼Œé€Ÿåº¦: {speed:.1f} è¡Œ/ç§’ï¼Œç”¨æ—¶: {elapsed:.1f}ç§’")

                    # æ‰¹é‡å¤„ç†
                    if len(batch_lines) >= BATCH_SIZE:
                        processed_records += process_batch(
                            batch_lines, item_stats, item_daily_counts,
                            global_daily_counts, action_map, get_date_from_timestamp
                        )
                        processed_lines += len(batch_lines)
                        batch_lines = []

                except Exception as e:
                    print(f"å¤„ç†è¡Œæ—¶å‡ºé”™: {line}, é”™è¯¯: {e}")

            # å¤„ç†å‰©ä½™è®°å½•
            if batch_lines:
                processed_records += process_batch(
                    batch_lines, item_stats, item_daily_counts,
                    global_daily_counts, action_map, get_date_from_timestamp
                )
                processed_lines += len(batch_lines)

            # å…³é—­å†…å­˜æ˜ å°„
            mmapped_file.close()

    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: æ•°æ®æ–‡ä»¶ {seq_file_path} æœªæ‰¾åˆ°ã€‚è¯·æ£€æŸ¥è·¯å¾„æˆ–ç¯å¢ƒå˜é‡é…ç½®ã€‚")
        return
    except Exception as e:
        print(f"âŒ å¤„ç†æ–‡ä»¶æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return

    data_processing_time = time.time() - start_time
    print(f"âœ… æ•°æ®èšåˆå®Œæˆï¼Œå¤„ç†äº† {processed_lines} è¡Œï¼Œ{processed_records} æ¡è®°å½•ï¼Œç”¨æ—¶: {data_processing_time:.1f}ç§’")
    print("ğŸ”¢ å¼€å§‹è®¡ç®—å„é¡¹æŒ‡æ ‡...")

    calc_start_time = time.time()
    results = []

    # æ‰¹é‡è®¡ç®—ï¼Œå‡å°‘é‡å¤æ“ä½œ
    total_items = len(item_stats)
    item_ids = list(item_stats.keys())

    # æ‰¹é‡å¤„ç†ç‰©å“ï¼Œæ¯1000000ä¸ªç‰©å“å¤„ç†ä¸€æ¬¡
    for i in range(0, total_items, 1000000):
        batch_ids = item_ids[i:i + 1000000]
        batch_results = []

        for item_id in batch_ids:
            stats = item_stats[item_id]
            all_timestamps = stats['all_timestamps']

            # ä½¿ç”¨numpyè¿›è¡Œå¿«é€Ÿç»Ÿè®¡è®¡ç®—
            if all_timestamps:
                all_timestamps_array = np.array(all_timestamps)

                # è·å–å†å²æ€»é‡
                total_exposures = len(stats['exposures'])
                total_clicks = len(stats['clicks'])
                total_conversions = len(stats['conversions'])

                # ä½¿ç”¨numpyè¿›è¡Œå¿«é€Ÿè®¡ç®—
                start_time_ts = float(all_timestamps_array.min())
                end_time_ts = float(all_timestamps_array.max())
                avg_all_time_ts = float(all_timestamps_array.mean())

                avg_day = get_date_from_timestamp(avg_all_time_ts)

                # å¿«é€Ÿè·å–å½“å¤©æ•°æ®
                exposures_on_avg_day = item_daily_counts[item_id]['exposures'].get(avg_day, 0)
                clicks_on_avg_day = item_daily_counts[item_id]['clicks'].get(avg_day, 0)
                conversions_on_avg_day = item_daily_counts[item_id]['conversions'].get(avg_day, 0)

                # è·å–å½“å¤©å…¨å±€ç»Ÿè®¡
                global_exposures = global_daily_counts['exposures'].get(avg_day, 0)
                global_clicks = global_daily_counts['clicks'].get(avg_day, 0)
                global_conversions = global_daily_counts['conversions'].get(avg_day, 0)

                # å¿«é€Ÿè®¡ç®—ç™¾åˆ†æ¯”
                exposure_pct = (exposures_on_avg_day / global_exposures * 100) if global_exposures > 0 else 0
                click_pct = (clicks_on_avg_day / global_clicks * 100) if global_clicks > 0 else 0
                conversion_pct = (conversions_on_avg_day / global_conversions * 100) if global_conversions > 0 else 0

                metrics_on_avg_day = {
                    'absolute_counts': {
                        'exposures': exposures_on_avg_day,
                        'clicks': clicks_on_avg_day,
                        'conversions': conversions_on_avg_day,
                    },
                    'global_counts_on_day': {
                        'exposures': global_exposures,
                        'clicks': global_clicks,
                        'conversions': global_conversions,
                    },
                    'percentage_of_global': {
                        'exposures_pct': f"{exposure_pct:.2f}%",
                        'clicks_pct': f"{click_pct:.2f}%",
                        'conversions_pct': f"{conversion_pct:.2f}%",
                    }
                }
            else:
                # ç©ºæ•°æ®çš„é»˜è®¤å€¼
                start_time_ts = None
                end_time_ts = None
                avg_all_time_ts = None
                total_exposures = 0
                total_clicks = 0
                total_conversions = 0
                metrics_on_avg_day = {
                    'absolute_counts': {
                        'exposures': 0,
                        'clicks': 0,
                        'conversions': 0,
                    },
                    'global_counts_on_day': {
                        'exposures': 0,
                        'clicks': 0,
                        'conversions': 0,
                    },
                    'percentage_of_global': {
                        'exposures_pct': "0.00%",
                        'clicks_pct': "0.00%",
                        'conversions_pct': "0.00%",
                    }
                }

            # æ„å»ºç»“æœ
            batch_results.append({
                'item_id': item_id,
                'exposure_start_ts': start_time_ts,
                'exposure_end_ts': end_time_ts,
                'exposure_avg_ts': avg_all_time_ts,
                'metrics_on_avg_day': metrics_on_avg_day,
                'total_counts': {
                    'exposures': total_exposures,
                    'clicks': total_clicks,
                    'conversions': total_conversions,
                    'all_actions': len(all_timestamps),
                }
            })

        results.extend(batch_results)

        # æ¯å¤„ç†1000000ä¸ªç‰©å“æ‰“å°ä¸€æ¬¡è¿›åº¦
        processed_count = min(i + 1000000, total_items)
        if processed_count % 10000000 == 0 or processed_count == total_items:
            elapsed = time.time() - calc_start_time
            progress = processed_count / total_items * 100
            speed = processed_count / elapsed if elapsed > 0 else float('inf')
            print(f"  è®¡ç®—è¿›åº¦: {processed_count}/{total_items} ({progress:.1f}%), é€Ÿåº¦: {speed:.1f} item/ç§’")

    calc_time = time.time() - calc_start_time
    total_time = time.time() - start_time
    print(f"ğŸ“Š æŒ‡æ ‡è®¡ç®—å®Œæˆï¼Œå…±å¤„ç† {len(results)} ä¸ªç‰©å“ï¼Œè®¡ç®—ç”¨æ—¶: {calc_time:.1f}ç§’")
    print(f"â±ï¸  æ€»ç”¨æ—¶: {total_time:.1f}ç§’ (æ•°æ®å¤„ç†: {data_processing_time:.1f}ç§’, æŒ‡æ ‡è®¡ç®—: {calc_time:.1f}ç§’)")

    # å¿«é€Ÿæ’åº
    print("ğŸ”„ æ­£åœ¨æ’åºç»“æœ...")
    sort_start = time.time()
    results.sort(key=lambda x: x['total_counts']['all_actions'], reverse=True)
    sort_time = time.time() - sort_start
    print(f"âœ… æ’åºå®Œæˆï¼Œç”¨æ—¶: {sort_time:.1f}ç§’")

    # æ‰“å°é¢„è§ˆï¼ˆé™åˆ¶æ•°é‡ä»¥æé«˜é€Ÿåº¦ï¼‰
    print("\n--- åˆ†æç»“æœé¢„è§ˆ (æŒ‰æ€»è¡Œä¸ºæ•°æ’åº) ---")
    preview_count = min(10, len(results))
    for res in results[:preview_count]:
        if res['exposure_avg_ts'] is not None:
            avg_day_str = get_date_from_timestamp(res['exposure_avg_ts']).isoformat()
        else:
            avg_day_str = "N/A"

        if res['exposure_start_ts'] is not None:
            start_day_str = get_date_from_timestamp(res['exposure_start_ts']).isoformat()
            end_day_str = get_date_from_timestamp(res['exposure_end_ts']).isoformat()
        else:
            start_day_str = "N/A"
            end_day_str = "N/A"

        print(f"\n[ Item ID: {res['item_id']} ]")
        print(f"  æ‰€æœ‰è¡Œä¸ºå¼€å§‹/ç»“æŸæ—¶é—´: {start_day_str} / {end_day_str}")
        print(f"  å¹³å‡æ›å…‰æ—¶é—´: {avg_day_str}")
        counts = res['total_counts']
        print(
            f"  å†å²æ€»é‡: æ›å…‰={counts['exposures']}, ç‚¹å‡»={counts['clicks']}, è½¬åŒ–={counts['conversions']}, æ€»è¡Œä¸º={counts['all_actions']}")
        metrics = res['metrics_on_avg_day']
        abs_counts = metrics['absolute_counts']
        global_counts = metrics['global_counts_on_day']
        pcts = metrics['percentage_of_global']
        print(
            f"  åœ¨å¹³å‡æ—¥çš„æŒ‡æ ‡: è¯¥ç‰©å“({abs_counts['exposures']}/{abs_counts['clicks']}/{abs_counts['conversions']}) / å…¨å±€({global_counts['exposures']}/{global_counts['clicks']}/{global_counts['conversions']}) = å æ¯”({pcts['exposures_pct']}/{pcts['clicks_pct']}/{pcts['conversions_pct']})")

    # ä¿å­˜ç»“æœ
    print("\nğŸ’¾ æ­£åœ¨ä¿å­˜ç»“æœ...")
    save_start = time.time()
    output_file = output_dir / 'item_exposure_data.pkl'
    output_dir.mkdir(parents=True, exist_ok=True)
    try:
        with open(output_file, 'wb') as f:
            pickle.dump(results, f, protocol=pickle.HIGHEST_PROTOCOL)
        save_time = time.time() - save_start
        print(f"âœ… å®Œæ•´åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}ï¼Œä¿å­˜ç”¨æ—¶: {save_time:.1f}ç§’")

        # æ€§èƒ½æ€»ç»“
        final_total_time = time.time() - start_time
        print(f"\nğŸ¯ æ€§èƒ½æ€»ç»“:")
        print(f"  æ€»ç”¨æ—¶: {final_total_time:.1f}ç§’")
        print(f"  æ•°æ®å¤„ç†: {data_processing_time:.1f}ç§’ ({data_processing_time / final_total_time * 100:.1f}%)")
        print(f"  æŒ‡æ ‡è®¡ç®—: {calc_time:.1f}ç§’ ({calc_time / final_total_time * 100:.1f}%)")
        print(f"  æ’åº: {sort_time:.1f}ç§’ ({sort_time / final_total_time * 100:.1f}%)")
        print(f"  ä¿å­˜: {save_time:.1f}ç§’ ({save_time / final_total_time * 100:.1f}%)")
        print(f"  å¤„ç†é€Ÿåº¦: {line_count / final_total_time:.0f} è¡Œ/ç§’")
        print(f"  è®°å½•å¤„ç†é€Ÿåº¦: {processed_records / final_total_time:.0f} è®°å½•/ç§’")

    except Exception as e:
        print(f"\nâŒ ä¿å­˜ç»“æœæ–‡ä»¶å¤±è´¥: {e}")


def process_batch(batch_lines, item_stats, item_daily_counts,
                  global_daily_counts, action_map, get_date_func):
    """å¤„ç†ä¸€æ‰¹è®°å½•ï¼Œè¿”å›å¤„ç†çš„è®°å½•æ•°"""
    records_count = 0
    for line in batch_lines:
        try:
            user_sequence = ujson.loads(line)

            # å¤„ç†å½“å‰ç”¨æˆ·çš„æ‰€æœ‰è®°å½•
            for record in user_sequence:
                _, item_id, _, _, action_type, timestamp = record
                records_count += 1

                # åªå¤„ç†æœ‰æ•ˆçš„ã€å·²çŸ¥çš„è¡Œä¸ºç±»å‹
                if item_id is not None and action_type in action_map:
                    action_name = action_map[action_type]

                    # ä½¿ç”¨ç¼“å­˜çš„æ—¥æœŸè½¬æ¢
                    day_key = get_date_func(timestamp)

                    # è®°å½•æ—¶é—´æˆ³åˆ°æ‰€æœ‰è¡Œä¸ºåˆ—è¡¨
                    item_stats[item_id]['all_timestamps'].append(timestamp)

                    # è®°å½•ç‰¹å®šè¡Œä¸ºç±»å‹çš„æ—¶é—´æˆ³
                    if action_name in item_stats[item_id]:
                        item_stats[item_id][action_name].append(timestamp)
                    else:
                        # å¦‚æœè¡Œä¸ºç±»å‹ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°åˆ—è¡¨
                        item_stats[item_id][action_name] = [timestamp]

                    # ä½¿ç”¨Counterè¿›è¡Œå¿«é€Ÿè®¡æ•°
                    item_daily_counts[item_id][action_name][day_key] += 1
                    global_daily_counts[action_name][day_key] += 1

        except Exception as e:
            print(f"å¤„ç†è®°å½•æ—¶å‡ºé”™: {line}, é”™è¯¯: {e}")

    return records_count


# =============================================================================
# ä¸»ç¨‹åºå…¥å£
# =============================================================================

def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºå¤„ç†å‘½ä»¤è¡Œå‚æ•°å’Œå¯åŠ¨åˆ†æã€‚"""
    parser = argparse.ArgumentParser(
        description='ç‰©å“è¡Œä¸ºåˆ†æè„šæœ¬ï¼Œè®¡ç®—æ›å…‰ã€ç‚¹å‡»ã€è½¬åŒ–ç­‰å…³é”®æŒ‡æ ‡ (æ€§èƒ½ä¼˜åŒ–ç‰ˆ)ã€‚'
    )
    parser.add_argument(
        '--mode',
        type=str,
        default='action_analysis',
        choices=['action_analysis'],
        help='æŒ‡å®šè¿è¡Œæ¨¡å¼ã€‚å½“å‰ä»…æ”¯æŒ "action_analysis"ã€‚'
    )

    args = parser.parse_args()

    if args.mode == 'action_analysis':
        paths = get_data_paths()
        print("=" * 60)
        print("=== ç‰©å“æ›å…‰ä¸è¡Œä¸ºåˆ†æ (æ€§èƒ½ä¼˜åŒ–ç‰ˆ) ===")
        print(f"åºåˆ—æ–‡ä»¶: {paths['seq_file']}")
        print(f"è¾“å‡ºç›®å½•: {paths['output_dir']}")
        print("=" * 60)

        if not paths['seq_file'].exists():
            print(f"é”™è¯¯: åºåˆ—æ–‡ä»¶ä¸å­˜åœ¨ {paths['seq_file']}")
            sys.exit(1)

        analyze_item_actions(paths['seq_file'], paths['output_dir'])
    else:
        print(f"âŒ æœªçŸ¥æ¨¡å¼: {args.mode}")
        sys.exit(1)


if __name__ == "__main__":
    main()